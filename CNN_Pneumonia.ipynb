{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO/19IVuvaBXm/4jXr7dns4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Artshouldterrify/btp/blob/main/CNN_Pneumonia.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BTP Project- Pneumonia X-Ray Classification\n",
        "***\n",
        "This project aims to classify X-Rays into two categories - ones afflicted by Pneumonia and ones that are normal, via the use of an ensemble of Deep Learning Convolutional Neural Networks (CNNs).\n",
        "\n",
        "We use two datasets, one containing images of 'Normal' and 'Pneumonia' X-Rays and one containing masks of chest X-Rays for segmentation purposes. We preprocess our images using a U-net based segmentation model and apply data augumentation to it, training the U-net model on the mask dataset.\n",
        "\n",
        "We then train several CNNs and combine them using an ensemble to form a classifier. We achieve a testing accuracy of 94.6% and a recall of 98.205%.\n",
        "\n"
      ],
      "metadata": {
        "id": "ixF3XT1AXBPx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing the data\n",
        "***\n",
        "The datasets are hosted on Kaggle, and are imported using the *kaggle* library.\n",
        "\n",
        "**Note**: This code requires an uploaded kaggle.json file to work."
      ],
      "metadata": {
        "id": "fM5MT5WpYQUR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install kaggle\n",
        "!mkdir ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "z4qmCHHqY3tv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download paultimothymooney/chest-xray-pneumonia"
      ],
      "metadata": {
        "id": "FGK4Nr3EY6Q3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/chest-xray-pneumonia.zip"
      ],
      "metadata": {
        "id": "TlpNRIbeY7ts"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# installing a library we'll need that's not automatically availlable in the Colab environment.\n",
        "\n",
        "!pip install tf-clahe"
      ],
      "metadata": {
        "id": "ciFl-qsXZBQW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting up data pipelines\n",
        "***\n",
        "To feed in data to our models, we set up data pipelines using Keras preprocessing. We create a custom function that'll allow us to create a separate pipeline for each model we make, allowing us to perform specific and modifiable preprocessing for each model."
      ],
      "metadata": {
        "id": "1gr9vOcuZIjc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# required imports\n",
        "\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import cv2\n",
        "import tf-clahe\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay"
      ],
      "metadata": {
        "id": "RJscJT2QZoyw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loading the images\n",
        "train_dir = \"/content/chest_xray/train\"\n",
        "test_dir = \"/content/chest_xray/test\"\n",
        "\n",
        "# pipeline generator\n",
        "def datagen(func):\n",
        "  train_gen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
        "              preprocessing_function=func,\n",
        "              horizontal_flip = True,\n",
        "              vertical_flip = True,\n",
        "              rotation_range = 0.2,\n",
        "              width_shift_range = 0.1,\n",
        "              height_shift_range = 0.1,\n",
        "              shear_range = 0.2,\n",
        "              zoom_range = 0.2)\n",
        "\n",
        "  test_gen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
        "              preprocessing_function=func)\n",
        "\n",
        "  train_set = train_gen.flow_from_directory(train_dir, class_mode = \"binary\", batch_size = 32, target_size = (224, 224))\n",
        "  test_set = test_gen.flow_from_directory(test_dir, class_mode = \"binary\", batch_size = 32, target_size = (224, 224), shuffle = False, seed = 10)\n",
        "  return train_set, test_set"
      ],
      "metadata": {
        "id": "J5GmbGHoZrLd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualizing the data\n",
        "***"
      ],
      "metadata": {
        "id": "VzN78WzvZ80-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# loading the images into a Dataset object\n",
        "\n",
        "batch_size, h, w = 32, 224, 224\n",
        "train_ds = tf.keras.utils.image_dataset_from_directory(\"/content/chest_xray/train\", image_size=(h, w), subset='training', validation_split=0.2, seed=12, batch_size=batch_size, labels=\"inferred\", label_mode=\"int\")\n",
        "test_ds = tf.keras.utils.image_dataset_from_directory(\"/content/chest_xray/test\", image_size=(h, w), seed=12, batch_size=batch_size, labels=\"inferred\", label_mode=\"int\")\n",
        "val_ds = tf.keras.utils.image_dataset_from_directory(\"/content/chest_xray/train\", image_size=(h, w), subset='validation', validation_split=0.2, seed=12, batch_size=batch_size, labels=\"inferred\", label_mode=\"int\")\n",
        "class_names = train_ds.class_names\n",
        "train_ds.class_names, test_ds.class_names, val_ds.class_names"
      ],
      "metadata": {
        "id": "4XGccyipZ_2b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plotting 9 random images with their labels\n",
        "\n",
        "plt.figure(figsize=(10,10))\n",
        "for images, labels in train_ds.take(1):\n",
        "  for i in range(9):\n",
        "    ax = plt.subplot(3, 3, i+1)\n",
        "    plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
        "    plt.title(train_ds.class_names[labels[i]])\n",
        "    plt.axis(\"off\")"
      ],
      "metadata": {
        "id": "2MsDa1v0aBbu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# checking the size of each image\n",
        "\n",
        "for images, labels in train_ds.take(1):\n",
        "  for i in range(1):\n",
        "    print(images[i].numpy().shape)"
      ],
      "metadata": {
        "id": "n0F80ML9aGOn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Defining the model\n",
        "***\n",
        "\n",
        "We train multiple models based on differing pre-trained models and pick the best performing ones to use in an Ensemble. The code for training these models is largely the same, and we indicate the nuances which allow us to change the pre-trained model being used."
      ],
      "metadata": {
        "id": "X2i_25b3aSRB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We firstly import the Segmentation mask-generating model. We include this pre-processing step as a layer above our our actual model."
      ],
      "metadata": {
        "id": "bENcVnz_bL20"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# loading in the kaggle-trained segmentation model\n",
        "\n",
        "unet_mask = tf.keras.models.load_model(\"/content/drive/MyDrive/newUnet (1).h5\")\n",
        "unet_mask.trainable = False\n",
        "unet_mask.summary()"
      ],
      "metadata": {
        "id": "Yvc_hGsWbC6b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# defining a custom layer to generate a mask\n",
        "\n",
        "class maskLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self, **kwargs):\n",
        "    super(maskLayer, self).__init__(**kwargs)\n",
        "  def call(self, inputs):\n",
        "    grayscale_image = tf.image.rgb_to_grayscale(inputs)\n",
        "    img_clahe = tf_clahe.clahe(grayscale_image, clip_limit=10.0)\n",
        "    mask = unet_mask(grayscale_image)\n",
        "    mask = tf.where(mask>0.5, 1.0, 0.0)\n",
        "    masked_output = inputs * mask\n",
        "    return masked_output"
      ],
      "metadata": {
        "id": "0SSDLUyabbJU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We test this mask layer on a random image taken from our dataset."
      ],
      "metadata": {
        "id": "yIOvBHH5bnBv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# testing the mask layer\n",
        "\n",
        "transf = maskLayer()\n",
        "\n",
        "plt.figure(figsize=(10,10))\n",
        "for images, labels in train_ds.take(1):\n",
        "  print(images[0].shape)\n",
        "  ax = plt.subplot(2,1,1)\n",
        "  im = transf(tf.reshape(images[0], (1,224,224,3))).numpy()\n",
        "  plt.imshow(images[0].numpy().astype(\"uint8\"))\n",
        "  ax = plt.subplot(2,1,2)\n",
        "  im = tf.reshape(im, (224,224,3))\n",
        "  plt.imshow(im.numpy().astype(\"uint8\"))"
      ],
      "metadata": {
        "id": "llheiZVvbuNv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We're now ready to define our actual model. We've shown here an example of a model based off MobileNetV2, which can be changed to, for example, ResNetV2 by simply modifying the model defined as the *base_model*, using different modules of *tf.keras.applications*. As an example, we include code of ResNetV2 in the next cell."
      ],
      "metadata": {
        "id": "u3fRQIn0bzuL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# model\n",
        "\n",
        "base_model = tf.keras.applications.mobilenet_v2.MobileNetV2(include_top=False, weights='imagenet', input_shape=(224,224,3))\n",
        "base_model.trainable = False\n",
        "inputs = tf.keras.Input(shape=(224,224,3))\n",
        "x = maskLayer()(inputs)\n",
        "x = base_model(x, training=False)\n",
        "x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
        "x = tf.keras.layers.Dense(128, activation=\"relu\")(x)\n",
        "x = tf.keras.layers.Dense(64, activation=\"relu\")(x)\n",
        "x = tf.keras.layers.Dense(32, activation=\"relu\")(x)\n",
        "outputs = tf.keras.layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "model = tf.keras.Model(inputs, outputs)\n",
        "model.summary()\n",
        "\n",
        "model.compile(optimizer = tf.keras.optimizers.Adam(), loss = \"binary_crossentropy\", metrics = [\"accuracy\"])"
      ],
      "metadata": {
        "id": "3J1wjDBMb3C1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model\n",
        "\n",
        "base_model = tf.keras.applications.resnet.ResNet50V2(include_top=False, weights='imagenet', input_shape=(224,224,3))\n",
        "base_model.trainable = False\n",
        "inputs = tf.keras.Input(shape=(224,224,3))\n",
        "x = maskLayer()(inputs)\n",
        "x = base_model(x, training=False)\n",
        "x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
        "x = tf.keras.layers.Dense(128, activation=\"relu\")(x)\n",
        "x = tf.keras.layers.Dense(64, activation=\"relu\")(x)\n",
        "x = tf.keras.layers.Dense(32, activation=\"relu\")(x)\n",
        "outputs = tf.keras.layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "model = tf.keras.Model(inputs, outputs)\n",
        "model.summary()\n",
        "\n",
        "model.compile(optimizer = tf.keras.optimizers.Adam(), loss = \"binary_crossentropy\", metrics = [\"accuracy\"])"
      ],
      "metadata": {
        "id": "rXSngWsqc189"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now generate our training and testing data pipelines using the function we defined previously. We note here that the preprocessing function passed as an argument to our function must correspond to the pre-trained model you're using.\n",
        "\n",
        "We additionally define a callback to stop training if validation performance hasn't increased for a set amount of training iterations."
      ],
      "metadata": {
        "id": "H3yTISbVdIU8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# training and testing set\n",
        "tr_set, te_set = datagen(tf.keras.applications.mobilenet_v2.preprocess_input)\n",
        "\n",
        "# callback\n",
        "early_stopping_callbacks = tf.keras.callbacks.EarlyStopping(patience = 5,\n",
        "                                                            restore_best_weights = True,\n",
        "                                                            verbose = 1)"
      ],
      "metadata": {
        "id": "ful_SiwVdLip"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# training the model\n",
        "history_mv2 = model.fit(tr_set, epochs = 50, validation_data = te_set, steps_per_epoch = 100,\n",
        "                              callbacks = [early_stopping_callbacks])"
      ],
      "metadata": {
        "id": "OnsbED0Ldyr4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluation testing accuracy\n",
        "model.evaluate(te_set)"
      ],
      "metadata": {
        "id": "PxVHL-UheJBE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loss curve\n",
        "l = np.concatenate([history_mv2.history['loss']])\n",
        "acc = np.concatenate([history_mv2.history['accuracy']])\n",
        "lv = np.concatenate([history_mv2.history['val_loss']])\n",
        "accv = np.concatenate([history_mv2.history['val_accuracy']])\n",
        "\n",
        "plt.plot(l, label=\"Loss\")\n",
        "plt.plot(acc, label=\"Accuracy\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "plt.plot(lv, label=\"Val_Loss\")\n",
        "plt.plot(accv, label=\"Val_Accuracy\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "nO6TpCoueSjs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fine-tuning the models by unfreezing some layers of the base model."
      ],
      "metadata": {
        "id": "So-Z-gKUeVNe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# fine tune\n",
        "base_model = model.layers[1]\n",
        "base_model.trainable = True\n",
        "\n",
        "fine_tune_at = 45\n",
        "\n",
        "for layer in base_model.layers[:fine_tune_at]:\n",
        "  layer.trainable = False\n",
        "model.compile(loss = \"binary_crossentropy\", optimizer=tf.keras.optimizers.Adam(learning_rate=0.00001), metrics=[\"accuracy\"])\n",
        "\n",
        "history_mb_ft = model.fit(tr_set, epochs = 100, validation_data = te_set, steps_per_epoch = 100,\n",
        "                              callbacks = [early_stopping_callbacks])"
      ],
      "metadata": {
        "id": "5BGzFzuqfFFH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loss curve for fine-tuning\n",
        "l = np.concatenate([history_mb_ft.history['loss']])\n",
        "acc = np.concatenate([history_mb_ft.history['accuracy']])\n",
        "lv = np.concatenate([history_mb_ft.history['val_loss']])\n",
        "accv = np.concatenate([history_mb_ft.history['val_accuracy']])\n",
        "\n",
        "plt.plot(l, label=\"Loss\")\n",
        "plt.plot(acc, label=\"Accuracy\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "plt.plot(lv, label=\"Val_Loss\")\n",
        "plt.plot(accv, label=\"Val_Accuracy\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "2VgcuxD3fheT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# testing the final model\n",
        "model.evaluate(test_ds)"
      ],
      "metadata": {
        "id": "A2ibuJzYf1Xz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now generate a Confusion Matrix, calculating different metrics for our model."
      ],
      "metadata": {
        "id": "VbOHIs8fh7UM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# confusion matrix\n",
        "preds = model.predict(te_set)\n",
        "y_pred = tf.where(preds<=0.5,0,1)\n",
        "\n",
        "labels = te_set.labels\n",
        "y_true = labels\n",
        "\n",
        "cm = confusion_matrix(y_pred, y_true)\n",
        "f = ConfusionMatrixDisplay(cm, display_labels=class_names)\n",
        "fig, ax = plt.subplots(figsize=(10,10))\n",
        "f.plot(ax=ax)"
      ],
      "metadata": {
        "id": "sUaB9B5iiCfR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# precision, recall, f1-score, accuracy\n",
        "p, r, f, b = sk.precision_score(y_pred, y_true, average=\"weighted\"), sk.recall_score(y_pred, y_true, average=\"weighted\"), sk.f1_score(y_pred, y_true, average=\"weighted\"), sk.balanced_accuracy_score(y_pred, y_true)\n",
        "p, r, f, b"
      ],
      "metadata": {
        "id": "Tti5_hfLiVs6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ensemble\n",
        "***\n",
        "We load the best models and combine them into an ensemble."
      ],
      "metadata": {
        "id": "n8OyoBYaipnk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# loading the models\n",
        "rv2 = tf.keras.models.load_model(\"/content/drive/MyDrive/rsnt_cs.h5\")\n",
        "mbv2 = tf.keras.models.load_model(\"/content/drive/MyDrive/mobilenet_model.h5\")\n",
        "eff = tf.keras.models.load_model(\"/content/drive/MyDrive/pretrain_eff (1).h5\")\n",
        "tr_set, te_set = datagen(tf.keras.applications.mobilenet_v2.preprocess_input)\n",
        "tr_set_eff, te_set_eff = datagen(tf.keras.applications.efficientnet_v2.preprocess_input)"
      ],
      "metadata": {
        "id": "Em4vuAPXjBYF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "r_pred, e_pred, m_pred = rv2.predict(te_set), eff.predict(te_set_eff), mbv2.predict(te_set)"
      ],
      "metadata": {
        "id": "ZNxtMKuEjDsg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Combining the models according to some weights, we ran multiple sets of weights to find the best ones using the following code."
      ],
      "metadata": {
        "id": "WHC3yX3xj0pC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_params = list()\n",
        "best_score = 0.0\n",
        "labels = te_set.labels\n",
        "y_true = labels\n",
        "A = np.arange(1, 10, 0.5)\n",
        "for a in A:\n",
        "  for b in A:\n",
        "    for c in A:\n",
        "      tot = (a*r_pred + b*e_pred + c*m_pred)/(a + b + c)\n",
        "      y_pred = tf.where(tot<=0.5,0,1)\n",
        "      score = sk.f1_score(y_true, y_pred)\n",
        "      if score > best_score:\n",
        "        best_score = score\n",
        "        best_params = [a,b,c]\n",
        "best_params, best_score"
      ],
      "metadata": {
        "id": "TBPFECEpj-dr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Final combination."
      ],
      "metadata": {
        "id": "gyX4v05qj_fR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_outputs = (3.0*r_pred + 1.0*e_pred + 2.0*m_pred)/(6.0)"
      ],
      "metadata": {
        "id": "-ZE1tZE-jG34"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally creating a Confusion Matrix and calculating metrics."
      ],
      "metadata": {
        "id": "vHtL68uKkTDF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# confusion matrix\n",
        "y_pred = tf.where(model_outputs<=0.5,0,1)\n",
        "labels = te_set_eff.labels\n",
        "y_true = labels\n",
        "\n",
        "cm = confusion_matrix(y_pred, y_true)\n",
        "f = ConfusionMatrixDisplay(cm, display_labels=class_names)\n",
        "fig, ax = plt.subplots(figsize=(10,10))\n",
        "f.plot(ax=ax)"
      ],
      "metadata": {
        "id": "s-2pFKuQkRiI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# precision, recall, f1-score, accuracy\n",
        "p, r, f, b = sk.precision_score(y_pred, y_true, average=\"weighted\"), sk.recall_score(y_pred, y_true, average=\"weighted\"), sk.f1_score(y_pred, y_true, average=\"weighted\"), sk.balanced_accuracy_score(y_pred, y_true)\n",
        "p, r, f, b"
      ],
      "metadata": {
        "id": "bSoaVcSQkngF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}